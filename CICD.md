# CI/CD Interview Questions and Answers

## CI/CD Workflow

We use the following tools orchestrated via **Jenkins** to implement a complete CI/CD pipeline:

We use GitHub as our source code repository, and our target platform is kubeenetes which is AWS EKS

Whenever a user commit a code, webhook triggers Jenkins pipeline, Jenkins starts Continous Integration.
As part of CI there are multiple stage:

- **Maven** for Java builds
- **SonarQube** for code quality analysis
- **AppScan/Trivy** for security scanning
- **Docker** for containerization
- **ArgoCD** for GitOps-based CD
- **Kubernetes** as the deployment platform
- **GitHub** as the version control system

### ***CI Stages***

1. **Checkout**: Pull the latest code committed by the developer
2. **Build & UT**: We will do build action along with the unit test cases, Compile the code using maven and run unit tests using Java testing framework
3. **Code Scan**: As part of this, we analyze code using SonarQube for security vulnerabilities and code quality issues
4. **Security Scan**: Use AppScan or Trivy to scan for vulnerabilities in dependencies or container layers
5. **Docker Image Build**: Build the container image for the application
6. **Image Scan**: Scan the Docker image for vulnerabilities, it is very important to do the image scaning, verify that image has any vulnerabilities in OS packages (e.g., Alpine, Ubuntu) and language-specific dependencies (e.g., pip, npm, Maven).
7. **Push to Registry**: Push the built image to a container registry (e.g., Docker Hub or AWS ECR)

### ***CD Workflow (GitOps)***

- We use **ArgoCD** for Continuous Deployment
- Once the image is pushed to the registry, **ArgoCD Image Updater** monitors the registry
- On detecting a new tag, it patches the Kubernetes manifest in the Git repository
- ArgoCD detects the change in Git and automatically deploys the application to the Kubernetes cluster

## **What are the branching strategies you use?**

Absolutely. We follow a structured `Git branching strategy` and `GitOps-based deployment` flow using `Jenkins, GitHub, ArgoCD`, and `AWS EKS` across four environments: `Dev, Staging, Pre-Prod, and Prod`.

We have main, develop, release and feature branches, 

Branch	               Purpose
feature/*	         Isolated development per feature/task
develop	            Integration of all features
release/x.y.z	      prep for release (UAT, testing)
main	               Stable production-ready code

- Developers create feature/* branches off develop.

- After completing a feature, they raise a PR to develop.

- Once multiple features are merged, we cut a release/* branch for testing and UAT.

- After approval, we merge release/* to main and tag it for production deployment.

üöß **1. feature/ ‚Üí Dev Environment**
   Push triggers Jenkins:
      Runs unit tests, linting, Trivy scan
      Builds Docker image: myapp:feature-add-login-abc123
      Pushes to ECR
      Jenkins updates gitops/dev/values.yaml with this tag

   ArgoCD syncs Dev: Deploys the new feature to Dev EKS namespace

   ‚úÖ Devs can test and iterate independently without affecting shared environments.

**üîÅ 2. Merge feature/ ‚Üí develop ‚Üí Staging Environment**
   After validation, dev creates a PR to merge into develop
   On merge:
      Jenkins builds new image: myapp:1.4.0-dev
      Pushes to ECR
      Updates gitops/staging/values.yaml with new tag

   Integration tests to verify services/components work together
   API contract validation
   Image vulnerability scanning with Trivy

   ArgoCD syncs Staging: This promotes the integrated feature(s) to the Staging environment

   ‚úÖ Here you test how multiple features work together. Also a place for integration/API testing.

**üöÄ 3. Create release/x.y.z ‚Üí Pre-Prod Environment**
   When you're ready for a release:
      we will create a release branch with version from develop
   Jenkins:
      Uses existing myapp:1.4.0-dev image or tags a stable one like 1.4.0-rc1
      Updates gitops/preprod/values.yaml with that image tag

   Regression tests to ensure nothing broke
   UAT is performed by QA/business users
   Performance testing with k6 or JMeter

   ArgoCD syncs Pre-Prod environment

   ‚úÖ This mirrors Prod closely for UAT, load tests, or final signoff

**üéØ 4. Merge release/x.y.z ‚Üí main ‚Üí Prod Environment**
   After Pre-Prod is signed off:
      we will merge release into main and tag version.
   Jenkins picks up the tag:
      Uses the prebuilt image 1.4.0
      Updates gitops/prod/values.yaml

   ArgoCD syncs to Prod cluster

   ‚úÖ Clean, versioned deployment with full traceability and audit trail.

## Q1: What are the different ways to trigger Jenkins pipelines?

Jenkins pipelines can be triggered in several ways:

- **Poll SCM**: Jenkins periodically checks Git for changes
- **GitHub Webhook**: GitHub sends a push event to Jenkins to trigger a build
- **Manual Trigger**: Users can manually start a pipeline from the Jenkins UI
- **API Trigger**: Use a REST API call to start the pipeline

## Q2: What components need to be backed up in Jenkins and how?

"In our Jenkins setup, I follow a reliable and automated backup strategy focused on resilience and recovery.

We back up the entire $JENKINS_HOME directory, which includes job configurations, plugins, credentials, secrets, user settings, and build history.

The backup runs as a nightly cron job using tar or rsync, and the archive is encrypted and stored in a versioned S3 bucket for durability and offsite recovery.

To backup Jenkins:

- **Configuration**: Backup the `~/.jenkins` or `$JENKINS_HOME` directory
- **Plugins**: Backup the `plugins/` folder inside `$JENKINS_HOME`
- **Jobs**: Backup the `jobs/` folder for job definitions
- **User Content**: Archive custom files, scripts, and artifacts
- **Database** (if used): Use `mysqldump` or a similar tool to backup external databases

**Restore Strategy**
To restore Jenkins, I:
   Stop the Jenkins service
   Extract the backup archive to the $JENKINS_HOME directory
   Ensure correct file permissions:
      `chown -R jenkins:jenkins /var/lib/jenkins`
   Restart the Jenkins service

Jenkins comes back online with all previous jobs, configurations, and plugins intact.

## Q3: If we have 5 stages in a Jenkins pipeline and 5th stage having syntax error then what will happen if we run the pipeline?

The pipeline will fail even before any stage is executed, because Jenkins parses the whole Jenkins file at the start. Syntax errors are caught during the parse phase, not at runtime.

## Q4: What are some main differences between scripted and declarative pipeline?

The main differences between Declarative and Scripted pipelines are in their syntax and flexibility:

- Declarative pipelines use a structured, easy to write, read, and validate format
- Scripted pipelines are written in `groovy` language, providing more control and flexibility, which is useful for more complex logic and dynamic behavior

## Q5: What is the difference between code quality and code coverage?

- **Code Quality**: Refers to how clean, maintainable, and bug-free the code is. It's usually measured through static analysis tools like SonarQube, which detect issues such as code smells, complexity, or security vulnerabilities

- **Code Coverage**: Measures how much of the codebase is exercised by automated tests. Tools like JaCoCo or Istanbul help calculate what percentage of code is tested

## Q6: What is SonarQube quality gate and what are its default conditions?

The default quality gate in SonarQube is called `sonarway`. A quality gate is a set of conditions that a project must meet to be considered as having passed code quality standards.

Default conditions in `sonarway` quality gate include:
- At least 80% code coverage on new code
- No critical bugs or vulnerabilities
- No more than 3% duplicated code

## Q7: Where do you write the code/yaml file for pipeline?

I write our pipeline code in the root of the Git repository, using a `Jenkinsfile` for Jenkins or .yml files for GitLab/GitHub/Azure.

## Q8: How to schedule pipeline, let's say I have validated the pipeline with some update and I want to schedule it to stage/main branch, how to do?

In Jenkins, I schedule pipelines using the `cron` trigger inside the Jenkinsfile. After validating feature branch, we use scheduled jobs to promote to staging or main branch.

Example:
```groovy
cron('H 2 * * *')
```
```bash
pipeline {
  agent any
  triggers {
    cron('H 2 * * *') // Runs daily at 2 AM (randomized minute to avoid overload)
  }
  stages {
    stage('Deploy to Staging') {
      when {
        branch 'stage'
      }
      steps {
        echo 'Deploying validated pipeline to staging...'
        // deployment steps
      }
    }
  }
}
```

## Q9: What security measures/tools you were taken in your CI/CD pipeline?

Security is most important for any application or organization. I take it as one of the top priorities whenever I build or create something, whether in cloud or outside of cloud.

In CI/CD, I enforce many security measures:

1. **Credential & Secret Management**
   - Use Jenkins secret credentials to store API keys, passwords
   - Never store plain text secrets

2. **SAST (Static Code Analysis)**
   - Integrated with Jenkins pipeline to scan for:
     - Code quality issues
     - Vulnerabilities

3. **Image Scanning**
   - Use Trivy to scan all Docker images before push to ECR or DockerHub

4. **Access Control**
   - Implement RBAC
   - Provide least privilege principles

## Q10: How do you set approval in CD pipeline?

Approvals in a CD pipeline are used to pause the pipeline at critical stages (usually before deploying to production), requiring a manual review or sign-off by an authorized user or team. They help ensure compliance, security, and change control before code goes live.

Example in Jenkins ‚Äì Input Step in Declarative Pipeline:
```groovy
stage('Approval') {
  steps {
    input message: 'Deploy to production?', ok: 'Approve'
  }
}
```

## Q11: CI/CD pipeline needs rollback capability. How would you implement it?

Rollback is a critical part of a resilient CI/CD pipeline, especially in production. A good rollback strategy ensures you can revert to a previously working version quickly and safely in case of failures. There are 4 ways to implement rollback:

1. **GitOps-Based Rollback (ArgoCD or FluxCD)**
   - Your deployments are declared in Git (Helm values, Kustomize, YAML)
   - Rollback = revert to a previous Git commit, and ArgoCD syncs it automatically

2. **Docker Image Tag Pinning**
   - Each build pushes an immutable tag (e.g., `myapp:sha-abc123`)
   - Your pipeline records the tag used in prod
   - Rollback = re-deploy old image tag

3. **Helm Rollback**
   - If you use Helm in your CD:
     ```bash
     helm rollback myapp <REVISION_NUMBER>
     ```

4. **Kubernetes Native Rollback**
   - If using raw kubectl:
     ```bash
     kubectl rollout undo deployment/myapp
     kubectl rollout undo deployment/myapp --to-revision=2
     ```

## Q12: How to configure Jenkins to trigger Pipeline B automatically after Pipeline A?

1. **Use build Step in Pipeline A**  
   In Pipeline A's Jenkinsfile, trigger Pipeline B.

2. **Use "Build Triggers" in Pipeline B**  
   - Go to Pipeline B ‚Üí Configure  
   - Under Build Triggers, check ‚úÖ *Build after other projects are built*  
   - Enter the name of Pipeline A  
   - This is UI-based and works even without modifying Jenkinsfile.

## Q13: How would you implement dynamic stages in a Jenkinsfile based on environment variables?

Use a Scripted Pipeline or `script` blocks inside a Declarative Pipeline to define stages conditionally.

Based on environment variables or parameters, I conditionally define and execute stages. This allows the pipeline to behave differently for different environments like dev, staging, or prod.

## Q14: How will you check the vulnerability of your code?

1. **Static Application Security Testing (SAST)**
   - Use tools like SonarQube or Semgrep integrated into the CI pipeline
   - Catches code-level issues like insecure APIs, SQL injection risks, or hardcoded credentials

2. **Software Composition Analysis (SCA)**
   - Use tools like Snyk, OWASP Dependency-Check, or npm audit
   - Detects vulnerabilities in third-party libraries or open-source dependencies

3. **Container Image Scanning**
   - Use tools like Trivy or Grype
   - Scans Docker images for OS and package-level vulnerabilities before pushing to registry

4. **Secret Scanning**
   - Integrate tools like Gitleaks or GitGuardian
   - Ensures no secrets or keys are committed into the codebase

5. **Dynamic Testing (DAST)**
   - Use tools like OWASP ZAP in QA/staging environments
   - Simulates attacks and catches runtime vulnerabilities

## Q15: What measures would you take to secure sensitive information within your Jenkins pipeline scripts?

- Use **Jenkins Credentials Manager**
- Avoid echoing or printing secrets
- Use `withCredentials {}` blocks
- Mask sensitive outputs using `echo "***"` or `set +x`

## Q16: Describe how you would set up Jenkins to trigger builds based on GitHub pull requests.

To set up Jenkins to trigger builds on GitHub pull requests (PRs), I follow these steps:

1. **Install Required Plugins**
   - GitHub Integration Plugin
   - GitHub Pull Request Builder Plugin or GitHub Branch Source Plugin (for multibranch pipelines)

2. **Create GitHub Credentials in Jenkins**
   - Generate a GitHub personal access token
   - Add it to Jenkins credentials as a Secret Text or Username/Token

3. **Configure GitHub Webhooks**
   - Go to GitHub repository ‚Üí Settings ‚Üí Webhooks
   - Set webhook URL to `http://<jenkins-url>/github-webhook/`
   - Enable events:
     - Pull requests
     - Push
   - This ensures Jenkins reacts to PR creation and updates

4. **Configure Jenkins Jobs**

   a. **For Freestyle or Simple Jobs:**
   - Use GitHub Pull Request Builder Plugin
   - Configure job to use Git SCM with appropriate GitHub repo
   - Enable "GitHub hook trigger for GITScm polling"

   b. **For Multibranch Pipelines (Preferred Approach):**
   - Create a Multibranch Pipeline job
   - In Branch Sources section:
     - Select GitHub and provide repo credentials
     - Enable build strategies:
       - Build PRs from forks
       - Build PRs from origin
     - Configure webhook-based discovery with periodic scan (e.g., every 1 hour)

## Q17: How would you set up a build promotion strategy in Jenkins to promote builds through different stages (e.g., Dev, QA, Prod)?

- Use build parameters to select environments
- Tag or mark a build after successful QA
- Optionally use **Promoted Builds Plugin**

## Q18: How can you integrate test reports (e.g., JUnit, NUnit) into Jenkins to display test results?
To integrate test reports into Jenkins and display test results in the build UI, I configure the pipeline or job to publish reports using Jenkins' built-in support or plugins, depending on the test framework used.
Jenkins has native support for JUnit XML test reports.
**In a Pipeline, I use the junit step:**
```groovy
junit 'target/surefire-reports/*.xml'
```

## Q19: How do you configure Jenkins to automatically discover and run pipelines defined in source control?
To configure Jenkins to automatically discover and run pipelines from source control, I use the Multibranch Pipeline or GitHub Organization job types. This setup allows Jenkins to scan repositories, detect Jenkinsfiles in each branch or PR, and run them automatically.

**‚úÖ 1. Install Required Plugins**
   I make sure the following plugins are installed:
   Pipeline
   Git or GitHub Branch Source Plugin
   Bitbucket Branch Source or GitLab Plugin (based on SCM)
   Pipeline: Multibranch and Pipeline: SCM Step

**‚úÖ 2. Set Up a Multibranch Pipeline Job**
In Jenkins, I click "New Item", choose "Multibranch Pipeline", and give it a name.

Under Branch Sources, I configure:
   GitHub, Bitbucket, or Git repository URL.
   Add credentials (SSH key or access token).

Jenkins will then:
   Periodically scan the repository.
   Automatically detect new branches with a Jenkinsfile.
   Create jobs and run pipelines on those branches.

**‚úÖ 3. Use Webhooks for Instant Triggering**
I set up webhooks in the SCM (e.g., GitHub or Bitbucket) pointing to:
  ```bash
      http://<jenkins-url>/github-webhook/
   ```
This allows Jenkins to trigger builds immediately on push or PR creation, instead of waiting for polling.

## Q20: What steps would you take to manage long-running jobs that might time out or exceed resource limits?

I configure timeouts at both stage and pipeline level to avoid stuck or hanging jobs:

```groovy
timeout(time: 30, unit: 'MINUTES') {
  sh './heavy-task.sh'
}
```
This ensures that jobs are automatically aborted if they exceed expected duration.

‚úÖ 2. Run on Properly-Sized Agents
‚úÖ 3. Break Down into Smaller Stages or Jobs
‚úÖ 5. Use Parallel Execution Where Possible

## Q21: How would you visualize and monitor the progress of your Jenkins pipelines?

- Use **Blue Ocean Plugin** or **Pipeline Stage View**
- Monitor logs and artifacts under each build run

## Q22: A pipeline stage intermittently fails due to network issues. How would you add retry logic to handle such failures?

To handle intermittent network-related failures in Jenkins pipelines, I use the retry block, which retries a failing step or stage a specified number of times before marking it as failed. This helps avoid false negatives due to temporary issues like DNS errors, API timeouts, or flaky connections.

```groovy
retry(3) {
  sh 'curl -f http://unstable-service || exit 1'
}
```

## Q23: Describe how you would configure Jenkins to use multiple build agents for distributed builds.

To configure Jenkins for distributed builds using multiple agents, I follow a master-agent architecture, where the controller manages job orchestration and offloads actual job execution to remote agents. 
**‚úÖ 1. Set Up Jenkins Controller (Master)**
I install Jenkins on a central server and ensure it's properly secured.
The controller handles:
   Scheduling jobs
   Aggregating results
   Managing plugins and UI
   But does not run builds directly (for scalability and isolation)

‚úÖ 2. Provision Jenkins Agents
I set up build agents (also called "slaves") in one or more of the following environments:
   Bare-metal or VM-based agents
   Docker containers using the Docker Agent Plugin
   Kubernetes agents using the Kubernetes Plugin
   Cloud-based agents using EC2, Azure, or GCP plugins

‚úÖ 3. Connect Agents to Controller
I connect agents to the controller via:
   SSH (most common for Linux agents)
   JNLP (Java Web Start) for cross-platform or firewall-restricted environments
   Kubernetes auto-scaling pods for ephemeral agents

‚úÖ 4. Label Agents for Targeted Job Assignment
I assign labels to agents based on their capabilities (e.g., docker, java11, windows).
In the pipeline, I define:
   `agent { label 'docker' }`

This distributed build strategy allows Jenkins to scale horizontally, run jobs in parallel, isolate environments (e.g., OS, language, tools), and reduce total CI/CD execution time ‚Äî which is essential for large, fast-moving teams."

## Q24: How can you ensure that a particular job runs on a specific Jenkins agent or a group of agents?
To ensure that a job runs on a specific Jenkins agent or group of agents, I use agent labels and configure the job or pipeline to target those labels. This allows Jenkins to dispatch jobs only to agents that meet specific criteria like OS, tools, or hardware."


```groovy
pipeline {
  agent { label 'linux' }
}
```

## Q25: Describe how you would set up a Jenkins job that accepts parameters from the user before running the build.
To create a Jenkins job that accepts user input before starting the build, I configure the job to use build parameters. This allows users to select or enter values‚Äîlike environment, branch name, or version number‚Äîbefore triggering the pipeline.

I define parameters at the top of the Jenkinsfile:
```groovy
parameters {
  string(name: 'ENV', defaultValue: 'dev', description: 'Target environment')
}
```

Usage: `sh "./deploy.sh ${params.ENV}"`

## Q26: How would you configure Jenkins to send an email notification upon build completion, only if the build fails?
To send an email notification only when a Jenkins build fails, I configure the Email Extension Plugin or use the built-in email functionality in combination with post-build conditions."

**‚úÖ 1. Install & Configure Email Extension Plugin**
   Install the Email Extension Plugin (Email-ext).
   Configure SMTP settings under Manage Jenkins ‚Üí Configure System:

```groovy
post {
  failure {
    mail to: 'devops@company.com',
         subject: "Build Failed: ${env.JOB_NAME} #${env.BUILD_NUMBER}",
         body: "Check details: ${env.BUILD_URL}"
  }
}
```

## Q27: How can you archive and publish build artifacts in Jenkins for later use or download?
In Jenkins, I use the archiveArtifacts step to store important build outputs (like JARs, WARs, Docker files, logs, or reports) so they can be downloaded later or passed to downstream jobs.

```groovy
archiveArtifacts artifacts: '**/target/*.jar', fingerprint: true
```
After the build completes, artifacts appear under the "Archived Artifacts" section in the build page

## Q28: What is Blue Ocean in Jenkins and what are its features?

- Blue Ocean is a modern, visual interface for Jenkins pipelines
- It provides intuitive visualization of stages, steps, logs, and approvals
- Enables easier pipeline creation and editing

## Q29: How to Restart a Failed Build at Specific Stage
By default, Jenkins doesn't automatically support restarting a pipeline from a failed stage, but there are several strategies I use to achieve this behavior depending on the pipeline type and tools available."

Use checkpointing or:

```groovy
options {
  skipStagesAfterUnstable()
}
```

## Q30: How to Handle Timeouts
To handle timeouts in Jenkins, I use the timeout directive in both declarative and scripted pipelines. This ensures that long-running or stuck steps are automatically aborted, keeping the pipeline efficient and under control."

I wrap any stage or the entire pipeline using the timeout block:

```groovy
stage('Deploy') {
  options {
    timeout(time: 10, unit: 'MINUTES')
  }
  steps {
    sh './deploy.sh'
  }
}
```

## Scenario 1: Restarting a Failed Build at a Specific Stage

How can you configure a pipeline to restart from the failed stage instead of starting from the beginning?

Use checkpointing with options { skipStagesAfterUnstable() } or a stage-level restartable flag.

```bash

pipeline {
    options {
        skipStagesAfterUnstable()
    }
    stages {
        stage('Build') {
            steps {
                sh 'make build'
            }
        }
        stage('Test') {
            steps {
                sh 'make test'
            }
        }
    }
}
```
## How can you ensure that a build runs only on specific agents with certain labels?

Use a label in the agent section.
```bash

pipeline {
    agent { label 'linux-agent' }
    stages {
        stage('Build') {
            steps {
                sh 'build.sh'
            }
        }
    }
}
```



## Q31: Jenkins is failing to push a Docker image to the registry. How do you troubleshoot?
When Jenkins fails to push a Docker image to a registry (like AWS ECR, Docker Hub, or GitLab), I troubleshoot it methodically by checking authentication, permissions, network, and Docker image tagging."

‚úÖ 1. Check Jenkins Console Logs
First, I review the exact error in the build logs:
   Common errors include:

   denied: requested access to the resource is denied
   unauthorized: authentication required
   no basic auth credentials
   connection timed out
   repository does not exist

This helps pinpoint whether it's an auth issue, image tagging problem, or registry misconfiguration.

‚úÖ 2. Validate Docker Login in Jenkins
I confirm that Jenkins is logging in to the registry before pushing.

‚úÖ 3. Verify Jenkins Agent Docker Permissions
If Jenkins runs on a Linux agent, I make sure the Jenkins user is in the docker group:
   ```bash
      sudo usermod -aG docker jenkins
      sudo systemctl restart jenkins
```
‚úÖ 4. Check Docker Image Name and Tag
Incorrect naming causes push failures.
‚úÖ Ensure docker tag and docker push use the exact same reference.

## Q32: Explain how you would set up a multi-branch Jenkins pipeline for a GitHub repository.
To set up a multibranch pipeline in Jenkins for a GitHub repo, I configure Jenkins to automatically discover and manage pipelines for each branch. This ensures that feature, develop, release, and main branches each get their own pipeline execution without manually creating jobs."


üîπ 1. Install Required Plugins
   GitHub Branch Source Plugin
    Pipeline Plugin

üîπ 2. Create a Multibranch Pipeline Job
   Go to Jenkins ‚Üí New Item
   Enter a name, e.g., myapp-multibranch
   Choose Multibranch Pipeline

üîπ 3. Configure Branch Source (GitHub)
   Under Branch Sources, click Add Source ‚Üí GitHub
   Enter your:
   GitHub repo URL
   Credentials (Personal Access Token with repo scope)

üîπ 4. Configure Scan Triggers
   Under Scan Multibranch Pipeline Triggers:
   Check ‚úÖ "Periodically if not otherwise run"
   Set it to scan every 5 or 15 minutes: H/5 * * * *
   ‚úÖ This ensures Jenkins detects new branches and PRs automatically.

üîπ 5. Jenkinsfile in Each Branch
   Ensure every active branch (feature/*, develop, release/*, main) contains a Jenkinsfile at the root.
   This file defines that branch's CI/CD logic

## Q33: A Jenkins pipeline is randomly failing at the deployment stage to EKS. Logs show timeouts during kubectl apply.
üîç 1. Confirm Network Connectivity to EKS API
From the Jenkins agent:
`kubectl cluster-info`

If this command times out:

   Check if the Jenkins agent is in the same VPC or peered VPC as the EKS control plane
   Validate VPC Endpoint or private access settings (if using private EKS)
   Make sure security groups allow outbound HTTPS (port 443) to the EKS API

‚úÖ Fix: Ensure the Jenkins agent (EC2/worker) has network access to the EKS control plane.

üîê 2. Validate Authentication & kubeconfig Setup
Run:
   `kubectl get nodes`

 You may see Unable to connect to the server: timeout if:
   IAM role/token is expired
   aws eks update-kubeconfig hasn't been run recently
   You're missing aws-iam-authenticator or your kubeconfig context is wrong

üîÅ 4. Use Retry in Jenkins Pipeline
Random failures = transient issues. Add retry logic:

üß± 5. Confirm EKS Cluster & Node Health

## Q34: Your CI/CD pipeline has failed in Jenkins. How do you investigate?
‚úÖ 1. Check Console Output in Jenkins
   I start by reviewing the full build logs:
   Identify the exact stage and command where the failure occurred
   Look for clear error messages (e.g., permission denied, connection timeout, test failure)
   Trace back from the failure to see what led up to it

‚úÖ This gives immediate insight into whether it's a build, test, deploy, or environment issue.

‚úÖ 2. Identify Which Stage Failed

‚úÖ 3. Review Recent Code Changes
   If the job was triggered by a push/PR:\
   I check the Git diff or commit history

Did someone update a Dockerfile, a Kubernetes manifest, or change a critical test?


I review the Jenkins logs to locate the failing stage and error message, check related recent commits, and validate the environment, credentials, and external services. Based on the failure type, I either fix the code/config, restart a flaky agent, or roll back a breaking change ‚Äî and then re-trigger the build to verify the fix."

## Q35: If the pipeline fails due to existing resources, how do you handle RIP (Remove, Import, Plan)?
If a Jenkins pipeline fails during Terraform apply or deployment due to existing cloud resources, I follow the RIP method ‚Äî Remove, Import, Plan ‚Äî to resolve the conflict and bring the state back in sync without destroying existing infrastructure."

‚úÖ 1. Remove (from Terraform State)
   If a resource already exists in the cloud but Terraform tries to recreate it, the failure often indicates that:

   The resource exists outside of state (manually created or drifted)
   The pipeline is not aware of the current infrastructure
To fix it:
   ```bash
   terraform state rm <resource_address>
```
‚úÖ This removes the broken or outdated reference from the local/remote state without deleting the real resource

‚úÖ 2. Import (into Terraform State)
After removing, I import the existing resource back into Terraform so it can be managed going forward.
```bash
   terraform import <resource_address> <real_resource_id>
```
‚úÖ 3. Plan
Once imported, I run a clean terraform plan to:

Compare the current state to actual infrastructure
Ensure no unexpected changes will be made

If a pipeline fails due to a pre-existing resource, I use RIP ‚Äî Remove it from state, Import it properly with its real ID, and then Plan again to ensure consistency. This approach prevents accidental destruction and aligns Terraform with real infrastructure, especially in shared or legacy environments."

## Q36: Pipeline fails only on Tuesdays, no code changes ‚Äî how do you debug?
If a pipeline consistently fails on Tuesdays without any code changes, that suggests a time-based or environment-related issue, not a logic or syntax bug. I treat it like a temporal anomaly and investigate using a methodical, data-driven approach."

‚úÖ 1. Check the Failure Pattern
   I go to Jenkins ‚Üí Job History and look for:

   Specific timestamps (e.g., only 2AM builds?)
   Same stage failing every Tuesday?
   Same error message each time?

‚úÖ This confirms it's reproducible by time, not by code.

‚úÖ 2. Compare Jenkins Logs & Pipelines
   I compare:

   Last working Monday build vs. failing Tuesday build
   Environment variables
   Job logs and console outputs
```bash
   diff log-monday.txt log-tuesday.txt
```
"If a pipeline fails only on Tuesdays without code changes, I approach it as a temporal environment issue. I compare successful vs. failing runs, look for cron jobs, expiring tokens, rotating resources, or scheduled reboots. I reproduce the failure manually and add debug steps to isolate the root cause. These kinds of issues are often tied to external systems or overlooked automation triggers."

## Q37 How to optimize jenkins pipline to reduce build time
1. Use Parallel Execution
Split independent steps into parallel stages:
2. Avoid Unnecessary Builds
Use when conditions to skip stages based on branch, changes, or env:

## Q38 CI Pipeline Succeeds but App is Broken in Prod ‚Äî What Action Will You Take?

1. üî• Initiate Incident Response
Notify the team. Document the issue.
If user-facing and severe, consider triggering an automated rollback or manual redeploy of the last working version.
2. üß™ Check Prod Logs and Monitoring
View logs using ELK, CloudWatch, or your observability stack.
Check:
HTTP status codes (500s, 4xx)
Application logs
Metrics: memory, CPU, DB errors, timeouts
3. üîÅ Compare Staging and Production
Is staging missing any:
Environment variables?
Backend services?
Feature flags?
Confirm the artifact promoted to prod is the same tested in staging.
4. üîê Check Secrets and External Integrations
API keys, third-party integrations, database credentials ‚Äî misconfigured or rotated tokens can break production unexpectedly.
5. üßæ Check Infrastructure Differences
Is production using a different:
Kubernetes namespace?
Load balancer config?
Terraform state?
Sometimes prod has older AMIs, different volumes, or a custom security group.

## Q39 Pipeline Slows Down Over Time (Builds taking more time) ‚Äî How Will You Fix?

1. üìä Measure Stage Durations Over Time
Use CI tool metrics (Jenkins, GitHub Actions, GitLab) or integrate Prometheus/Grafana.
Identify which stage(s) are consuming more time ‚Äî code checkout, dependency resolution, test execution, build packaging, etc.
2. üì¶ Check Dependency Management
Over time, dependency trees can grow.
Use tools like:
mvn dependency:analyze
npm prune
Cache dependencies (e.g., ~/.m2, node_modules) between runs to avoid full re-downloads.
3. üíæ Enable Layered and Incremental Builds
Avoid cleaning entire workspace unless necessary (mvn clean install can be expensive).
Use incremental build options:
Gradle: --build-cache
Bazel: native caching
Docker: use layers wisely and avoid invalidating cache
4. üßº Clean Up Disk and Workspace
Runners may accumulate:
Gigabytes of build artifacts
Old Docker images and volumes
Use scheduled jobs or add cleanup logic:
docker system prune -af
5. üöÄ Use Parallelism and Matrix Builds
Split long test suites or build steps using:
strategy.matrix in GitHub Actions
parallel stages in Jenkins pipelines
6. ‚öôÔ∏è Review Runner/Agent Resource Utilization
Check CPU, memory, disk I/O on build agents.
Use autoscaling runners or move to faster instance types if needed.

üß† Real-World Example
We noticed our build time increased from 7 to 19 minutes over 6 months.
Root cause:

Increased number of tests without parallel execution
Outdated Docker layers not using cache
‚úÖ Fixes implemented:

Introduced parallel test stages
Refactored Dockerfile to leverage cache better
Cleaned up unused images on runners weekly

## I want my deployment to be implemented to specific workloads or regions, that update shouldn't go to other parts.